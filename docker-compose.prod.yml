# Docker Compose configuration for production environment
# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up

services:
  # Production Solana Validator (optimized)
  solana-validator:
    environment:
      START_VALIDATOR: "true"
      SKIP_DEPLOYMENT: "false"
      RUST_LOG: "info"
      HEALTH_CHECK_VERBOSE: "false"
    restart: always
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
    profiles:
      - production
      - prod

  # Production Contract Deployer (run once)
  contact:
    environment:
      RUST_LOG: "info"
      BUILD_ONLY: "false"
      VERIFY_VERBOSE: "false"
    restart: "no"  # Don't restart after successful deployment
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
    profiles:
      - deployment
      - production
      - prod

  # Production API Gateway
  api-gateway:
    build:
      context: .
      dockerfile: docker/api-gateway/Dockerfile  # Use production Dockerfile
      target: production
    environment:
      RUST_LOG: "info"
      LOG_LEVEL: info
      ENVIRONMENT: production
      ENABLE_CORS: "false"
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
        reservations:
          memory: 512M
    profiles:
      - production
      - prod

  # Production Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
    environment:
      NODE_ENV: production
      VITE_APP_ENV: production
      VITE_ENABLE_DEVTOOLS: "false"
      VITE_HOT_RELOAD: "false"
      VITE_SOLANA_RPC_URL: http://localhost:8899
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - production
      - prod

  # Production PostgreSQL with performance optimizations
  postgres:
    environment:
      POSTGRES_DB: p2p_energy_trading
      POSTGRES_USER: p2p_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-p2p_secure_password}
      # Performance optimizations
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c work_mem=4MB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
        reservations:
          memory: 512M
    profiles:
      - production
      - prod

  # Production Redis with persistence
  redis:
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    profiles:
      - production
      - prod

  # Production Kafka with optimizations
  kafka:
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      # Production optimizations
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
        reservations:
          memory: 512M
    profiles:
      - production
      - prod

  # Production TimescaleDB with optimizations
  timescaledb:
    command: >
      postgres
      -c shared_preload_libraries=timescaledb
      -c max_connections=100
      -c shared_buffers=128MB
      -c effective_cache_size=512MB
      -c work_mem=2MB
      -c maintenance_work_mem=32MB
      -c timescaledb.max_background_workers=8
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
        reservations:
          memory: 512M
    profiles:
      - production
      - prod

  # Production monitoring with security
  grafana:
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-secure_admin_password}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY:-secure_secret_key}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_INSTALL_PLUGINS: grafana-clock-panel
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    profiles:
      - production
      - prod
      - monitoring

  prometheus:
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
        reservations:
          memory: 512M
    profiles:
      - production
      - prod
      - monitoring

  # Production Smart Meter Simulator
  smart-meter-simulator:
    restart: always
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
    profiles:
      - production
      - prod
      - simulators

  # Production Oracle Simulator
  oracle-simulator:
    restart: always
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
    profiles:
      - production
      - prod
      - simulators

  # Production Nginx reverse proxy and load balancer
  nginx:
    image: nginx:alpine
    container_name: p2p-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - api-gateway
      - grafana
    networks:
      - p2p-network
    restart: always
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production
      - prod

# Production-specific volumes with backup configurations
volumes:
  postgres_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/lib/p2p/postgres
  redis_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/lib/p2p/redis
  kafka_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/lib/p2p/kafka
  grafana_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/lib/p2p/grafana
  prometheus_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/lib/p2p/prometheus

# Production network with custom subnet
networks:
  p2p-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16
    driver_opts:
      com.docker.network.bridge.name: p2p-prod-bridge